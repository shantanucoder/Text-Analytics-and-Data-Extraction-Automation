Data integration and big data performance using Elasticsearch
HomeOur Success StoriesData integration and big data performance using ElasticsearchOur Success StoriesITData integration and big data performance using ElasticsearchByAjay Bidyarthy-July 13, 20225521Client BackgroundClient:A Leading Tech Firm in the USAIndustry Type:IT & ConsultingServices:Software, Business Solutions, ConsultingOrganization Size:200+Project ObjectiveMigrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.Project DescriptionThe client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.Our SolutionSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash.Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data.Identify the code in the backend that needs to be changed.Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this.Testing Postgres and Elasticsearch performance.Project DeliverablesSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Pipeline i.e; logstash fileNew working backend code for elasticsearchCommands to check elastic data.Customizable logstash pipelineTools usedElasticsearchPostmanKibanaLogstashPythonJavascriptAmazon Web ServicesPostgresDockerGit BucketGithubLanguage/techniques usedJavascriptJsonDomain-Specific Language for elasticsearchbashSkills usedElasticsearch query knowledgePostgres query knowledgeNetworkingJavascriptBackend web stackDatabases usedPostgresElasticsearchWeb Cloud Servers usedAmazon Web Services (AWS)What are the technical Challenges Faced during Project ExecutionSometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs.How the Technical Challenges were SolvedTo solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.Business ImpactEarlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.Project SnapshotsPrevious articleWeb Data ConnectorNext articleAuvik, Connectwise integration in GrafanaAjay BidyarthyRELATED ARTICLESMORE FROM AUTHORIntegrating Machine Learning Code into Kubeflow Pipeline – Kuberflow MLOps KubernetesFacial Recognition Attendance SystemFace Recognition Using DeepFaceMOST POPULAR INSIGHTSAdvanced Data Visualization Solutions for Monitoring Key Business Metrics with Integrated,...August 25, 2024How to Overcome Your Fear of Making MistakesAugust 23, 2020Cloud-Based Web Application for Financial Data Processing and Visualization of S&P...August 25, 2024Clinical Trial: Big Data & AnalyticsMay 22, 2019Load moreRECOMMENDED INSIGHTSShould people wear fabric gloves? Seeking evidence regarding the differential transfer...Internet Demand’s Evolution, Communication Impact, and 2035’s Alternative PathwaysHow Coronavirus Impact on the Hospitality IndustryChallenges and Opportunities of Big Data in Healthcare